---
title: 'Lab Report 1'
author: "Mitheysh Asokan, Jason (Eungjoo) Kim"
geometry: margin=.75in
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
    theme: cosmo
header-includes:
- \usepackage{graphicx}
- \usepackage{color}
graphics: yes
fontsize: 11pt
---

<!-- You can check templates from this website and change html theme: https://www.datadreaming.org/post/r-markdown-theme-gallery/ -->
<!-- It won't affect the PDF or Word. -->

```{r echo=FALSE, message=FALSE, error=FALSE}

library('tidyverse')
library('gridExtra')
library('caret')
library('ISLR')

```



## Question 1: Linear Regression

Chosen Dataset: spotify_songs

Output Variable:
danceability

Input Variables:
playlist_genre , speechiness, energy, duration_ms


### 1.1. (10 pts)

Give basic insights into your numeric variable you have picked as output variable using one categorical variable you selected.

- What are the min / max values and median of the output variable, $Y$?

```{r}

songs <- read.csv('spotify_songs.csv')

max(songs$danceability)
min(songs$danceability)
median(songs$danceability)

```

- What is median of the output value among different classes of the categorical variable you picked? You must use `group_by` and `summarize` functions.

```{r}
group_by(songs, playlist_genre) %>%
  summarise(median.danceability=median(danceability))
```


### 1.2. (10 pts)

Visualize the variables you selected.

- Draw histogram of the numeric variables you selected.

```{r error=FALSE, message=FALSE}
g1 <- ggplot(data = songs) +
  geom_histogram(aes(x = danceability))

g2 <- ggplot(data = songs) +
  geom_histogram(aes(x = speechiness))

g3 <- ggplot(data = songs) +
  geom_histogram(aes(x = energy))

g4 <- ggplot(data = songs) +
  geom_histogram(aes(x = duration_ms))

grid.arrange(g1,g2,g3,g4, ncol=2)

```



- Draw distribution of the output variable $Y$ with respect to the different classes of your categorical variable. The plot must somehow show the distributional differences among different classes. You can use boxplot, histogram, or other visuals (e.g. density ringes).

```{r error=FALSE, message=FALSE}
songs$playlist_genre <- as.factor(songs$playlist_genre)

ggplot(songs, aes(x=playlist_genre, y=danceability, colour=playlist_genre)) +
  geom_boxplot()
```

- Draw scatter plot between one of your numeric inputs and the output variable. Discuss whether the plot indicate a relation, if it is linear, if there are outliers? Feel free to remove the outlier. Feel free to transform the data.

```{r error=FALSE, message=FALSE}
ggplot(songs, aes(x=duration_ms, y=danceability)) +
  geom_point(alpha=0.2)
```

The danceability of a song is clearly dependent to its duration. The relationship is evident, since the dancebility of a track is falling when its duration is rising.

The relationship is somewhat linear based on the scatterplot.

### 1.3. (15 pts)

Using the all dataset, fit a regression:

1. Using the one numeric input variable fit a simple regression model.

  - Write down the model.
  - Fit the regression line.
  - Summarize the output.
  - Plot the input and output variables in a scatter plot and add the predicted values as a line.
  - Interpret the results. Is it a good fit? Is your input variable good in explaining the outputs?


$HDanceability_i$ = $\beta_0$ + $\beta_1$$Duration_i$ + $\epsilon_i$


```{r}
fit1 <- lm(danceability ~ duration_ms, data = songs)
```


```{r}
summary(fit1)
```


```{r}
preds <- predict(fit1)
ggplot(songs, aes(x=duration_ms, y=danceability)) +
  geom_point(alpha=.3, size=2) +
  geom_line(aes(y=preds), colour="blue")
```

The line is appropriate fit (very low Mean Root Squared), that is downard sloping. Which means that danceability gradually falls as the duration of the song increases. This explanation is in line with how exhausted a dancer is getting with time. The longer the song, the more exhausted the dancer gets, which leads to stoppage. 

According to the summary, the data for danceability is strongly correlated with duration. 

The p-values are very close to zero and they are significant (due to the ***).

The f-stat is also very high for the degrees of freedom used.

The RMSE is also very low.

2. Using all your input variables, fit a multiple linear regression model

   - Write down the model
   - Fit the regression line and summarize the output
   - Interpret the results. Is it a good fit? Are the input variables good in explaining the outputs?


$HDanceability_i$ = $\beta_0$ + $\beta_1$$Duration_i$ + $\beta_2$$Energy_i$ + $\beta_3$$Speechiness_i$ + $\beta_4$$PlaylistGenre_i$ + $\epsilon_i$


```{r}
fit2 <- lm(danceability ~ duration_ms + energy + speechiness + playlist_genre, data = songs)
```

```{r}
summary(fit2)
```

The fit is appropriate (low Root Mean Squared, but it is getting better as we add more input variables), which means that danceability related to duration, energy, speechiness, and genre of the song.

The p-values are very close to zero and they are significant (due to the ***). Except for the R&B genre, which only score 2 asterisks for coefficient significance. This is expected as the genre is not intended for dancing.

The f-stat is also very high for the degrees of freedom used.

The RMSE is also very low.


3. Now, do the same things as you did, but this time add an interaction between one categorical and one numeric variable.
   - Write down the model, fit to the data, summarize and interpret the results.

$HDanceability_i$ = $\beta_0$ + $\beta_1$$Duration_i$ + $\beta_2$$Energy_i$ + $\beta_3$$Speechiness_i$ + $\beta_4$$PlaylistGenre_i$ + $\beta_5$$PlaylistGenre_i$$Speechiness_i$ + $\epsilon_i$


```{r}
fit3 <- lm(danceability ~ duration_ms + energy + speechiness + playlist_genre + playlist_genre:speechiness, data = songs)
```

```{r}
summary(fit3)
```

The fit is appropriate (low Root Mean Squared, but it is getting better as we add more input variables), which means that danceability related to duration, energy, speechiness, genre of the song, and the speechiness of each genre.

The p-values are very close to zero and they are significant (due to the ***). Except for the R&B genre, which only score 1 asterisk for coefficient significance (This is expected as the genre is not intended for dancing) and the speechiness of the latin, pop, and r&b genres (Which is also expected, as the latin and pop songs tend to focus on the music instead of the lyric to get people dancing to them).

The f-stat is also very high for the degrees of freedom used.

The RMSE is also very low.

4. Which model you fit is the best in predicting the output variable? Which one is the second and third best? Rank the models based on their performance.

```{r}
s1 <- sigma(fit1)
s2 <- sigma(fit2)
s3 <- sigma(fit3)

result <- c(s1, s2, s3)
result
```

RANKING: fit3, fit2, fit1

### 1.4. (15 pts)

In this section, you will do the same you did in 1.3, but this time you will first split the data into train and test.

- Select seed to fix the random numbers you will generate using `set.seed(...)`.

```{r}
set.seed(100)
```

- Split your data into test and train sets with 20/80 test-train ratio.

```{r}
train_size <- floor(0.8 * nrow(songs))
train_inds <- sample(1:nrow(songs), size = train_size)
test_inds  <- setdiff(1:nrow(songs), train_inds)

train <- songs[ train_inds , ]
test  <- songs[ test_inds , ]
```

- Fit the model to the train set and evaluate the how well the model performed on test set.

```{r}

fit1 <- lm(danceability ~ duration_ms, data = train)
fit2 <- lm(danceability ~ duration_ms + energy + speechiness + playlist_genre, data = train)
fit3 <- lm(danceability ~ duration_ms + energy + speechiness + playlist_genre + playlist_genre:speechiness, data = train)

pred1 <- predict(fit1, newdata=test)
pred2 <- predict(fit2, newdata=test)
pred3 <- predict(fit3, newdata=test)
```

- Which model performed the best on test set? Rank the models based ion their performance.

```{r}
rmse1 <- RMSE(pred1, test$danceability)
rmse2 <- RMSE(pred2, test$danceability)
rmse3 <- RMSE(pred3, test$danceability)

result <- c(rmse1,rmse2,rmse3)
result
```

- Is the rank the same as the one you had in 1.3?

Yes.


\pagebreak

## Question 2: Gradient Descent Algorithm (By hand)

In case you want to take a picture (screenshot) of your notebook (tablet), you can use the below lines to embed the image to the output PDF file:


```{r}
knitr::include_graphics('question2.jpg')
```

\pagebreak

## Question 3. Gradient Descent Algorithm


### 3.1. Get familiar

You will use horsepower as input variable and miles per gallon (mpg) as output:

1. Plot the scatterplot between `mpg` ($Y$) and `horsepower` ($X$).
    - Is the relationship positive or negative? Does mpg increase or reduce as horsepower increases?
    - Is the relationship linear?

```{r}
ggplot(Auto, aes(x=horsepower, y=mpg)) +
  geom_point(alpha=0.2)
```

Negative relationship. MPG decreases as Horsepower increases.

The relationship seems to be non-linear, as the scatterplot depicts a curved shaped best-fit line.


2. Plot the scatterplot between `log(mpg)` and `log(horsepower)`.
    - Is the relationship positive or negative?
    - Is the relationship linear?

  ```{r}
ggplot(Auto, aes(x=log(horsepower), y=log(mpg))) +
  geom_point(alpha=0.2)
```  

Negative relationship. log(MPG) decreases as log(Horsepower) increases.

The relation seems to linear, as the scatterplot depict a straight best-fit line.


3. Which of the two versions is better for linear regression?

The second version is better for linear regression, as the data is better predicted using a straight downward sloping, best-fit line.


### 3.2. Fill in the code

The code below estimates the coefficients of linear regression using gradient descent algorithm. If you are given a single linear regression model;

$$Y = \beta_0 + \beta_1 X $$

where $Y=[Y_1,\dots,Y_N]^T$ and $X=[X_1,\dots,X_N]^T$ are output and input vectors containing the observations.

The algorithm estimates the parameter vector $\theta = [\beta_0,\beta_1]$ by starting with an arbitrary $\theta_0$ and adjusting it with the gradient of the loss function as:

$$\theta := \theta + \frac \alpha N X^T(Y - \theta X)$$

where $\alpha$ is the step size (or learning rate) and $(Y - \theta X)^T X$ is the gradient. At each step it calculates the gradient of the loss and adjusts the parameter set accordingly.

### 3.3. Run GDA


1. Run the code with the above parameters. How many iterations did it take to estimate the parameters?

```{r}
GDA <- function(x, y, theta0, alpha = 0.01, epsilon = 1e-8, max_iter=25000){

  # Inputs
  # x      : The input variables (M columns)
  # y      : Output variables    (1 column)
  # theta0 : Initial weight vector (M+1 columns)

  x     <- as.matrix(x)
  y     <- as.matrix(y)
  N     <- nrow(x)
  i     <- 0
  theta <- theta0
  x     <- cbind(1, x) # Adding 1 as first column for intercept
  imprv <- 1e10
  cost  <- (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y)
  delta <- 1
  while(imprv > epsilon & i < max_iter){
    i <- i + 1
    grad <- (t(x) %*% (y-x %*% theta))
    theta <- theta + (alpha / N) * grad
    cost  <- append(cost, (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y))
    imprv <- abs(cost[i+1] - cost[i])
    if((cost[i+1] - cost[i]) > 0) stop("Cost is increasing. Try reducing alpha.")
  }
  if (i==max_iter){print(paste0("maximum interation ", max_iter, " was reached"))} else {
    print(paste0("Finished in ", i, " iterations"))
  }

  return(theta)
}

plot_line <- function(theta) {
  ggplot(Auto, aes(x=log(horsepower),y=log(mpg))) +
    geom_point(alpha=.7) +
    geom_abline(slope = theta[2], intercept = theta[1], colour='firebrick') +
    ggtitle(paste0('int: ', round(theta[1],2), ', slope: ', round(theta[2],2)))
}

x <- log(Auto$horsepower)
y <- log(Auto$mpg)
theta0 <- c(1,1)
theta   <- GDA(x, y, theta0, alpha = 0.05, epsilon = 1e-5)

plot_line(theta)
```


2. Reduce epsilon to `1e-6`, set `alpha=0.05` run the code.
    - How many iterations did it take to estimate the parameters?
    - Does the result improve? Why or why not?

```{r}
x <- log(Auto$horsepower)
y <- log(Auto$mpg)
theta0 <- c(1,1)
theta   <- GDA(x, y, theta0, alpha = 0.05, epsilon = 1e-6)
plot_line(theta)
```

Result does not improve. The condition (imprv > epsilon) is going to take longer to terminate, as the epsilon value is now smaller.

3. Reduce alpha to `alpha=0.01`
   - How many iterations did it take?
   - Did the resulting line change? Why or why not?

```{r}
x <- log(Auto$horsepower)
y <- log(Auto$mpg)
theta0 <- c(1,1)
theta   <- GDA(x, y, theta0, alpha = 0.01, epsilon = 1e-6)

plot_line(theta)
```

The resulting line did change. The slope increased, hence the best fit line is not a bteer representation of the data points. The increased learning rate, slightly increased our predictions, which seems to be a step in the right dirextion.


4. Set alpha back to `alpha=0.05` and try `theta0=c(1,1)` vs. `theta0=c(1,-1)`:
   - How many iterations did it take? Which is less than the other?
   - Why starting with a negative slope have this effect?

```{r}
x <- log(Auto$horsepower)
y <- log(Auto$mpg)
theta0 <- c(1,1)
theta   <- GDA(x, y, theta0, alpha = 0.05, epsilon = 1e-6)
plot_line(theta)
theta0 <- c(1,-1)
theta   <- GDA(x, y, theta0, alpha = 0.05, epsilon = 1e-6)
plot_line(theta)
```

theta0=c(1,1) took 7531 iterations, while theta0=c(1,-1) took 7265 iterations. 

Starting with the negative slope is the accurate decision, since the data is downward sloping. Adding weight to the negative slope is rewarded with lower iterations to complete.

5. Reduce epsilon to `epsilon = 1e-8` and try `alpha=0.01`, `alpha=0.05` and `alpha=0.1`.
   - What effect does alpha have on iterations and resulting fitted line?
```{r}
x <- log(Auto$horsepower)
y <- log(Auto$mpg)
theta0 <- c(1,1)
theta   <- GDA(x, y, theta0, alpha = 0.01, epsilon = 1e-8)
plot_line(theta)
theta   <- GDA(x, y, theta0, alpha = 0.05, epsilon = 1e-8)
plot_line(theta)

#Commented out, as it seems to crash the GDA function. (Error in GDA, Cost is increasing. Try reducing Alpha.)
#theta   <- GDA(x, y, theta0, alpha = 0.1, epsilon = 1e-8)
#plot_line(theta)
```

Increasing the learning rate improves the predictions and the slope changes to better reflect the datapoints in the scatterplat. This can be clearly observed in the plots drawn for alpha = 0.01 and alpha = 0.05.

\pagebreak

## Question 4. BGD vs. SGD


1. Copy the BGD code and convert it into a Stochastic Gradient Descent algorithm by applying necessary changes inside. Name the code SGD.

```{r}
BGD <- function(x, y, theta0, alpha = 0.01, epsilon = 1e-8, max_iter=25000){
  
  # Inputs
  # x      : The input variables (M columns)
  # y      : Output variables    (1 column)
  # theta0 : Initial weight vector (M+1 columns)
  
  x     <- as.matrix(x)
  y     <- as.matrix(y) 
  N     <- nrow(x)
  i     <- 0
  theta <- theta0
  x     <- cbind(1, x) # Adding 1 as first column for intercept
  imprv <- 1e10
  cost  <- (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y)
  delta <- 1
  while(imprv > epsilon & i < max_iter){cost
    i <- i + 1
    grad <- 0
    for(j in 1:length(y)){
      grad_chng <- x[j, ] * c(y[j]-x[j, ] %*% theta)
      grad <- grad + grad_chng 
    }
    theta <- theta + (alpha / N) * grad
    cost  <- append(cost, (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y))
    imprv <- abs(cost[i+1] - cost[i])
    if((cost[i+1] - cost[i]) > 0) stop("Cost is increasing. Try reducing alpha.")
  }
  print(paste0("Stopped in ", i, " iterations"))
  
  cost <- cost[-1]
  return(list(theta,cost))
}
```

```{r}
SGD <- function(x, y, theta0, alpha = 0.01, epsilon = 1e-8, max_iter=25000){
  
  # Inputs
  # x      : The input variables (M columns)
  # y      : Output variables    (1 column)
  # theta0 : Initial weight vector (M+1 columns)
  
  x     <- as.matrix(x)
  y     <- as.matrix(y) 
  N     <- nrow(x)
  i     <- 0
  theta <- theta0
  x     <- cbind(1, x) # Adding 1 as first column for intercept
  imprv <- 1e10
  cost  <- (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y)
  delta <- 1
  while(i < max_iter){cost
    i <- i + 1
    grad <- 0
    new_cost <- 0
    for(j in 1:length(y)){
      random <- runif(1,min=1,max=length(y))
      grad_chng <- x[random, ] * c(y[random]-x[random, ] %*% theta)
      grad <- grad + grad_chng 
      theta <- theta + (alpha / N) * grad
      new_cost <- sum((1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y))
    }
    cost  <- append(cost, new_cost)
    imprv <- abs(cost[i+1] - cost[i])
  }
  print(paste0("Stopped in ", i, " iterations"))
  
  cost <- cost[-1]
  return(list(theta,cost))
}
```
2. Compare BGD and SGD.
  - Which algorithm yielded minimum loss?
  
The Batch Gradient Algorithm yielded minimum loss when comparing the results of 10 iterations (epochs). The BGD converged to the minima much quicker than SGD after 10 iterations. Given the fact that the training data set is small, and that SGD uses sum of all the data set to determine the next most optimal step, the step sizes tend to be bigger and are more optimized per epoch than that of SGD. Furthermore, BGD approach always strives for convergence towards the minima; while SGD fluctuates around the minima due to the existence of oscillations. 

From a pure SGD algorithm standpoint, there are more iterations required to converge to the optimal loss of 0 because the step sizes for SGD is smaller by nature as it is calculated for every training data point, as opposed to a training data set of BGD.

  - Plot the resulting losses. Does the resulting losses differ?

The following graph represents the comparison of BGD algorithm (black) vs. SGD algorithm (orange) at 10 iterations:

```{r}
res <- BGD(x, y, c(1, -1), alpha = 0.005, epsilon = 1e-5, max_iter = 10)
res2 <- SGD(x, y, c(1, -1), alpha = 0.005, epsilon = 1e-5, max_iter = 10)

theta_bgd <- res[[1]]
loss_bgd  <- res[[2]]

theta_sgd <- res2[[1]]
loss_sgd <- res2[[2]]

ggplot() + 
    geom_point(aes(x=1:length(loss_bgd), y=loss_bgd)) +
    geom_point(aes(x=1:length(loss_sgd), y=loss_sgd), color="orange") +
    labs(x='iteration') + labs(y='loss')
```

Here, we can see that the BGD approach actually converges to the minima much quicker than SGD, due to the fact that the step sizes are bigger per iteration. 

The following graph provides a more "zoomed-out" view of the comparison, where we compare BGD algorithm (black) vs. SGD algorithm (orange) at 60 iterations:

```{r}
res <- BGD(x, y, c(1, -1), alpha = 0.005, epsilon = 1e-5, max_iter = 60)
res2 <- SGD(x, y, c(1, -1), alpha = 0.005, epsilon = 1e-5, max_iter = 60)

theta_bgd <- res[[1]]
loss_bgd  <- res[[2]]

theta_sgd <- res2[[1]]
loss_sgd <- res2[[2]]

ggplot() + 
    geom_point(aes(x=1:length(loss_bgd), y=loss_bgd)) +
    geom_point(aes(x=1:length(loss_sgd), y=loss_sgd), color="orange") +
    labs(x='iteration') + labs(y='loss')
```

Here, we can we see the nature of "noisiness" of SGD, as a random data point is used to determine the subsequent step sizes. Although SGD algorithm converges to the minima, it stills shows signs of oscillations due to the stochistic nature of the algorithm. 